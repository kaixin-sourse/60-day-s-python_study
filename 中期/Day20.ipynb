{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 奇异值分解（SVD）全面解析\n",
    "## 一、奇异值分解概述\n",
    "奇异值分解是线性代数中一个重要的矩阵分解方法，对于任何矩阵，无论是结构化数据转化成的“样本 * 特征”矩阵，还是天然以矩阵形式存在的图像数据，都能进行等价的奇异值分解（SVD）。\n",
    "\n",
    "## 二、不必纠结的部分\n",
    "### 1. 线性代数概念回顾\n",
    "奇异值分解涉及诸多线性代数概念，不过对于很多实际应用场景，不深入掌握这些概念也不妨碍我们使用奇异值分解来解决问题。\n",
    "\n",
    "### 2. 奇异值推导\n",
    "奇异值的推导过程较为复杂，涉及大量的数学公式和证明。在实际应用中，我们更关注其应用而非推导过程，所以这部分也可不掌握。\n",
    "\n",
    "## 三、奇异值的强大应用\n",
    "### 1. 特征降维\n",
    "- 减小计算量 ：在结构化数据里，原本有 $m$ 个特征，通过奇异值分解，我们能选取保留前 $K$ 个奇异值及其对应的奇异向量，将数据降维成 $k$ 个新特征。新特征是原始特征的线性组合，捕捉了数据的主要方差信息。降维后的数据规模变小，用于机器学习模型（如分类、回归）时，能显著提高计算效率。\n",
    "- 可视化 ：高维数据难以直接可视化，通过奇异值分解降维到二维或三维，就能在平面或空间中直观展示数据分布，帮助我们更好地理解数据特征。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始数据形状: (150, 4)\n",
      "降维后数据形状: (150, 2)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 加载鸢尾花数据集\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 进行奇异值分解\n",
    "U, S, VT = np.linalg.svd(X_scaled)\n",
    "\n",
    "# 选择前 2 个奇异值进行降维\n",
    "K = 2\n",
    "X_reduced = np.dot(U[:, :K], np.diag(S[:K]))\n",
    "\n",
    "print(\"原始数据形状:\", X_scaled.shape)\n",
    "print(\"降维后数据形状:\", X_reduced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 数据重构\n",
    "- 重构信号 ：在信号处理领域，奇异值分解可以对信号矩阵进行分解。通过保留主要的奇异值和奇异向量重构信号，去除噪声干扰，恢复信号的主要特征。\n",
    "- 重构图像 ：对于图像数据，利用奇异值分解选取部分奇异值和奇异向量重构图像。既能在一定程度上压缩图像数据，又能保持图像的主要视觉特征，实现图像的有损压缩。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frobenius 范数相对误差: 0.20461653892093765\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# 加载鸢尾花数据集\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "\n",
    "# 数据标准化\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# 进行奇异值分解\n",
    "U, S, VT = np.linalg.svd(X_scaled)\n",
    "\n",
    "# 选择前 2 个奇异值进行重构\n",
    "K = 2\n",
    "S_k = np.zeros((X_scaled.shape[0], X_scaled.shape[1]))\n",
    "S_k[:K, :K] = np.diag(S[:K])\n",
    "X_reconstructed = np.dot(np.dot(U, S_k), VT)\n",
    "\n",
    "# 计算 Frobenius 范数相对误差\n",
    "error = np.linalg.norm(X_scaled - X_reconstructed) / np.linalg.norm(X_scaled)\n",
    "print(\"Frobenius 范数相对误差:\", error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、操作注意事项\n",
    "在进行 SVD 之前，通常要对数据进行标准化处理，使数据均值为 0，方差为 1。这样做能避免某些特征因量纲差异过大，对降维结果产生不合理的影响，保证奇异值分解的有效性和准确性。\n",
    "\n",
    "## 五、误差衡量\n",
    "对分解后的矩阵进行重构原始矩阵操作后，可通过计算 Frobenius 范数相对误差来衡量原始矩阵和重构矩阵的差异。该误差值能帮助我们判断保留不同数量奇异值时重构的效果，进而选择合适的 $K$ 值以达到最佳的应用效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
